{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gameOb():\n",
    "    def __init__(self,coordinates,size,intensity,channel,reward,name):\n",
    "        self.x = coordinates[0]\n",
    "        self.y = coordinates[1]\n",
    "        self.size = size\n",
    "        self.intensity = intensity\n",
    "        self.channel = channel\n",
    "        self.reward = reward\n",
    "        self.name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class gameEnv():\n",
    "    def __init__(self,size):\n",
    "        self.sizeX = size\n",
    "        self.sizeY = size\n",
    "        self.actions = 4\n",
    "        self.objects = []\n",
    "        a = self.reset()\n",
    "        plt.imshow(a,interpolation = \"nearest\")\n",
    "        \n",
    "    def reset(self):\n",
    "        self.objects = []\n",
    "        hero = gameOb(self.newPosition(),1,1,2,None,'hero')\n",
    "        self.objects.append(hero)\n",
    "        for i in range(4):\n",
    "            goal=gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "            self.objects.append(goal)\n",
    "            if i < 2:\n",
    "                hole = gameOb(self.newPosition(),1,1,0,-1,'fire')\n",
    "                self.objects.append(hole)\n",
    "        state = self.renderEnv()\n",
    "        self.state = state\n",
    "        return state\n",
    "    \n",
    "    def moveChar(self,direction):\n",
    "        hero = self.objects[0]\n",
    "        if direction == 0 and hero.y >=1:\n",
    "            hero.y -=1\n",
    "        if direction == 1 and hero.y <=self.sizeY-2:\n",
    "            hero.y +=1\n",
    "        if direction == 2 and hero.x >=1:\n",
    "            hero.x -=1\n",
    "        if direction == 3 and hero.y <=self.sizeX-2:\n",
    "            hero.x +=1    \n",
    "        self.objects[0] = hero\n",
    "        \n",
    "    def newPosition(self):\n",
    "        points = []\n",
    "        for t in itertools.product(range(self.sizeX),range(self.sizeY)):\n",
    "            points.append(t)\n",
    "        for objectA in self.objects:\n",
    "            points.remove((objectA.x,objectA.y))\n",
    "        location = np.random.choice(range(len(points)),replace = False)\n",
    "        return points[location]\n",
    "    \n",
    "    def checkGoal(self):\n",
    "        hero=self.objects[0]\n",
    "        others=self.objects[1:]\n",
    "        for other in others:\n",
    "            if hero.x == other.x and hero.y == other.y:\n",
    "                self.objects.remove(other)\n",
    "                if other.reward == 1:\n",
    "                    self.objects.append(gameOb(self.newPosition(),1,1,1,1,'goal'))\n",
    "                else:\n",
    "                    self.objects.append(gameOb(self.newPosition(),1,1,0,-1,'fire'))\n",
    "                return other.reward,False\n",
    "        return 0.0,False\n",
    "    \n",
    "    def renderEnv(self):\n",
    "        a = np.ones([self.sizeY+2,self.sizeX+2,3])\n",
    "        a[1:-1,1:-1,:] = 0\n",
    "        for item in self.objects:\n",
    "            a[item.y+1:item.y+item.size+1,item.x+1:item.x+item.size+1\n",
    "              ,item.channel] = item.intensity\n",
    "        b = scipy.misc.imresize(a[:,:,0],[84,84,1],interp='nearest')\n",
    "        c = scipy.misc.imresize(a[:,:,1],[84,84,1],interp='nearest')\n",
    "        d = scipy.misc.imresize(a[:,:,2],[84,84,1],interp='nearest') \n",
    "        a = np.stack([b,c,d],axis=2)\n",
    "        return a\n",
    "    \n",
    "    def step(self,action):\n",
    "        self.moveChar(action)\n",
    "        reward,done = self.checkGoal()\n",
    "        state = self.renderEnv()\n",
    "        return state,reward,done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFiCAYAAAAna2l5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmr9KhgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x135cc5bd518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env=gameEnv(size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size):\n",
    "        self.scalarInput = tf.placeholder(shape=[None,84*84*3],\n",
    "                                         dtype=tf.float32)\n",
    "        self.imageIn=tf.reshape(self.scalarInput,shape=[-1,84,84,3])\n",
    "        self.conv1 = tf.contrib.layers.convolution2d(\n",
    "            inputs=self.imageIn,num_outputs=32,\n",
    "            kernel_size=[8,8],stride=[4,4],\n",
    "            padding='VALID',biases_initializer=None)\n",
    "        self.conv2 = tf.contrib.layers.convolution2d(\n",
    "            inputs=self.conv1,num_outputs=64,\n",
    "            kernel_size=[4,4],stride=[2,2],\n",
    "            padding='VALID',biases_initializer=None)\n",
    "        self.conv3 = tf.contrib.layers.convolution2d(\n",
    "            inputs=self.conv2,num_outputs=64,\n",
    "            kernel_size=[3,3],stride=[1,1],\n",
    "            padding='VALID',biases_initializer=None)\n",
    "        self.conv4 = tf.contrib.layers.convolution2d(\n",
    "            inputs=self.conv3,num_outputs=512,\n",
    "            kernel_size=[7,7],stride=[1,1],\n",
    "            padding='VALID',biases_initializer=None)\n",
    "        self.streamAC,self.streamVC = tf.split(self.conv4,2,3)\n",
    "        self.streamA=tf.contrib.layers.flatten(self.streamAC)\n",
    "        self.streamV=tf.contrib.layers.flatten(self.streamVC)\n",
    "        self.AW=tf.Variable(tf.random_normal([h_size//2,env.actions]))\n",
    "        self.VW=tf.Variable(tf.random_normal([h_size//2,1]))\n",
    "        self.Advantage=tf.matmul(self.streamA,self.AW)\n",
    "        self.value=tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        self.Qout=self.value+tf.subtract(self.Advantage,tf.reduce_mean(\n",
    "            self.Advantage,reduction_indices=1,keep_dims=True))\n",
    "        self.predict=tf.argmax(self.Qout,1)\n",
    "        \n",
    "        self.targetQ=tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions=tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot=tf.one_hot(self.actions,env.actions,\n",
    "                                       dtype=tf.float32)\n",
    "        self.Q=tf.reduce_sum(tf.multiply(self.Qout,self.actions_onehot),\n",
    "                            reduction_indices=1)\n",
    "        self.td_error=tf.square(self.targetQ-self.Q)\n",
    "        self.loss=tf.reduce_mean(self.td_error)\n",
    "        self.updateModel=tf.train.AdamOptimizer(0.0001).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self,buffer_size=50000):\n",
    "        self.buffer=[]\n",
    "        self.buffer_size=buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer)+len(experience)>self.buffer_size:\n",
    "            popLength=len(self.buffer)+len(experience)-self.buffer_size\n",
    "            self.buffer[0:popLength]=[]\n",
    "        self.buffer.extend(experience)\n",
    "        \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),\n",
    "                         [size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def flattenState(states):\n",
    "    return np.reshape(states,[84*84*3])\n",
    "\n",
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars=len(tfVars)\n",
    "    qn_vars=total_vars//2\n",
    "    op_holder=[]\n",
    "    for idx,var in enumerate(tfVars[0:qn_vars]):\n",
    "#         print(var.value())\n",
    "        op_holder.append(tfVars[idx+qn_vars].assign((var.value()*tau)+\\\n",
    "                                    ((1-tau)*tfVars[idx+qn_vars].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "update_freq=4\n",
    "y=0.99\n",
    "startE=1\n",
    "endE=0.1\n",
    "anneling_steps=10000\n",
    "num_episodes=10000\n",
    "pre_train_steps=10000\n",
    "max_epLength=50\n",
    "load_model=False\n",
    "path=\"./dqn\"\n",
    "h_size=512\n",
    "tau=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Conv/weights/read:0\", shape=(8, 8, 3, 32), dtype=float32)\n",
      "Tensor(\"Conv_1/weights/read:0\", shape=(4, 4, 32, 64), dtype=float32)\n",
      "Tensor(\"Conv_2/weights/read:0\", shape=(3, 3, 64, 64), dtype=float32)\n",
      "Tensor(\"Conv_3/weights/read:0\", shape=(7, 7, 64, 512), dtype=float32)\n",
      "Tensor(\"Variable/read:0\", shape=(256, 4), dtype=float32)\n",
      "Tensor(\"Variable_1/read:0\", shape=(256, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "mainQN=Qnetwork(h_size)\n",
    "targetQN=Qnetwork(h_size)\n",
    "init =tf.global_variables_initializer()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "targetOps=updateTargetGraph(trainables,tau)\n",
    "\n",
    "buff=experience_buffer()\n",
    "\n",
    "e=startE\n",
    "stepDrop=(startE-endE)/anneling_steps\n",
    "\n",
    "rList=[]\n",
    "total_steps=0\n",
    "\n",
    "saver= tf.train.Saver()\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 25 ,average reward of last 25 episode 1.56\n",
      "episode 50 ,average reward of last 25 episode 0.8\n",
      "episode 75 ,average reward of last 25 episode 0.96\n",
      "episode 100 ,average reward of last 25 episode 1.96\n",
      "episode 125 ,average reward of last 25 episode 2.44\n",
      "episode 150 ,average reward of last 25 episode 1.96\n",
      "episode 175 ,average reward of last 25 episode 1.68\n",
      "episode 200 ,average reward of last 25 episode 0.84\n",
      "episode 225 ,average reward of last 25 episode 1.68\n",
      "episode 250 ,average reward of last 25 episode 1.12\n",
      "episode 275 ,average reward of last 25 episode 1.76\n",
      "episode 300 ,average reward of last 25 episode 1.76\n",
      "episode 325 ,average reward of last 25 episode 0.6\n",
      "episode 350 ,average reward of last 25 episode 0.52\n",
      "episode 375 ,average reward of last 25 episode 0.84\n",
      "episode 400 ,average reward of last 25 episode 1.08\n",
      "episode 425 ,average reward of last 25 episode 0.96\n",
      "episode 450 ,average reward of last 25 episode 0.36\n",
      "episode 475 ,average reward of last 25 episode 0.6\n",
      "episode 500 ,average reward of last 25 episode 0.24\n",
      "episode 525 ,average reward of last 25 episode 0.56\n",
      "episode 550 ,average reward of last 25 episode 0.6\n",
      "episode 575 ,average reward of last 25 episode 0.84\n",
      "episode 600 ,average reward of last 25 episode 0.76\n",
      "episode 625 ,average reward of last 25 episode 1.16\n",
      "episode 650 ,average reward of last 25 episode 0.8\n",
      "episode 675 ,average reward of last 25 episode 1.28\n",
      "episode 700 ,average reward of last 25 episode 1.16\n",
      "episode 725 ,average reward of last 25 episode 1.44\n",
      "episode 750 ,average reward of last 25 episode 0.48\n",
      "episode 775 ,average reward of last 25 episode 1.28\n",
      "episode 800 ,average reward of last 25 episode 0.96\n",
      "episode 825 ,average reward of last 25 episode 1.2\n",
      "episode 850 ,average reward of last 25 episode 1.36\n",
      "episode 875 ,average reward of last 25 episode 1.04\n",
      "episode 900 ,average reward of last 25 episode 0.64\n",
      "episode 925 ,average reward of last 25 episode 0.92\n",
      "episode 950 ,average reward of last 25 episode 2.0\n",
      "episode 975 ,average reward of last 25 episode 2.04\n",
      "episode 1000 ,average reward of last 25 episode 1.2\n",
      "Saved Model\n",
      "episode 1025 ,average reward of last 25 episode 1.12\n",
      "episode 1050 ,average reward of last 25 episode 1.6\n",
      "episode 1075 ,average reward of last 25 episode 1.2\n",
      "episode 1100 ,average reward of last 25 episode 1.36\n",
      "episode 1125 ,average reward of last 25 episode 1.04\n",
      "episode 1150 ,average reward of last 25 episode 1.04\n",
      "episode 1175 ,average reward of last 25 episode 1.44\n",
      "episode 1200 ,average reward of last 25 episode 1.56\n",
      "episode 1225 ,average reward of last 25 episode 0.84\n",
      "episode 1250 ,average reward of last 25 episode 1.28\n",
      "episode 1275 ,average reward of last 25 episode 1.12\n",
      "episode 1300 ,average reward of last 25 episode 2.04\n",
      "episode 1325 ,average reward of last 25 episode 1.36\n",
      "episode 1350 ,average reward of last 25 episode 1.56\n",
      "episode 1375 ,average reward of last 25 episode 1.48\n",
      "episode 1400 ,average reward of last 25 episode 1.92\n",
      "episode 1425 ,average reward of last 25 episode 1.48\n",
      "episode 1450 ,average reward of last 25 episode 1.32\n",
      "episode 1475 ,average reward of last 25 episode 2.24\n",
      "episode 1500 ,average reward of last 25 episode 1.84\n",
      "episode 1525 ,average reward of last 25 episode 2.64\n",
      "episode 1550 ,average reward of last 25 episode 1.76\n",
      "episode 1575 ,average reward of last 25 episode 1.92\n",
      "episode 1600 ,average reward of last 25 episode 1.8\n",
      "episode 1625 ,average reward of last 25 episode 2.16\n",
      "episode 1650 ,average reward of last 25 episode 2.76\n",
      "episode 1675 ,average reward of last 25 episode 2.24\n",
      "episode 1700 ,average reward of last 25 episode 2.84\n",
      "episode 1725 ,average reward of last 25 episode 3.28\n",
      "episode 1750 ,average reward of last 25 episode 3.44\n",
      "episode 1775 ,average reward of last 25 episode 3.08\n",
      "episode 1800 ,average reward of last 25 episode 4.52\n",
      "episode 1825 ,average reward of last 25 episode 2.8\n",
      "episode 1850 ,average reward of last 25 episode 4.32\n",
      "episode 1875 ,average reward of last 25 episode 4.04\n",
      "episode 1900 ,average reward of last 25 episode 3.36\n",
      "episode 1925 ,average reward of last 25 episode 4.68\n",
      "episode 1950 ,average reward of last 25 episode 4.96\n",
      "episode 1975 ,average reward of last 25 episode 5.32\n",
      "episode 2000 ,average reward of last 25 episode 5.56\n",
      "Saved Model\n",
      "episode 2025 ,average reward of last 25 episode 6.64\n",
      "episode 2050 ,average reward of last 25 episode 6.2\n",
      "episode 2075 ,average reward of last 25 episode 6.28\n",
      "episode 2100 ,average reward of last 25 episode 5.92\n",
      "episode 2125 ,average reward of last 25 episode 6.04\n",
      "episode 2150 ,average reward of last 25 episode 7.16\n",
      "episode 2175 ,average reward of last 25 episode 7.4\n",
      "episode 2200 ,average reward of last 25 episode 6.84\n",
      "episode 2225 ,average reward of last 25 episode 9.24\n",
      "episode 2250 ,average reward of last 25 episode 10.16\n",
      "episode 2275 ,average reward of last 25 episode 10.2\n",
      "episode 2300 ,average reward of last 25 episode 11.32\n",
      "episode 2325 ,average reward of last 25 episode 8.0\n",
      "episode 2350 ,average reward of last 25 episode 9.64\n",
      "episode 2375 ,average reward of last 25 episode 11.4\n",
      "episode 2400 ,average reward of last 25 episode 10.2\n",
      "episode 2425 ,average reward of last 25 episode 13.68\n",
      "episode 2450 ,average reward of last 25 episode 11.08\n",
      "episode 2475 ,average reward of last 25 episode 14.2\n",
      "episode 2500 ,average reward of last 25 episode 12.64\n",
      "episode 2525 ,average reward of last 25 episode 15.24\n",
      "episode 2550 ,average reward of last 25 episode 11.56\n",
      "episode 2575 ,average reward of last 25 episode 14.92\n",
      "episode 2600 ,average reward of last 25 episode 11.6\n",
      "episode 2625 ,average reward of last 25 episode 13.2\n",
      "episode 2650 ,average reward of last 25 episode 15.48\n",
      "episode 2675 ,average reward of last 25 episode 14.08\n",
      "episode 2700 ,average reward of last 25 episode 15.36\n",
      "episode 2725 ,average reward of last 25 episode 13.44\n",
      "episode 2750 ,average reward of last 25 episode 16.16\n",
      "episode 2775 ,average reward of last 25 episode 16.92\n",
      "episode 2800 ,average reward of last 25 episode 16.84\n",
      "episode 2825 ,average reward of last 25 episode 17.12\n",
      "episode 2850 ,average reward of last 25 episode 16.32\n",
      "episode 2875 ,average reward of last 25 episode 16.76\n",
      "episode 2900 ,average reward of last 25 episode 17.32\n",
      "episode 2925 ,average reward of last 25 episode 16.88\n",
      "episode 2950 ,average reward of last 25 episode 15.24\n",
      "episode 2975 ,average reward of last 25 episode 16.68\n",
      "episode 3000 ,average reward of last 25 episode 16.84\n",
      "Saved Model\n",
      "episode 3025 ,average reward of last 25 episode 15.92\n",
      "episode 3050 ,average reward of last 25 episode 17.16\n",
      "episode 3075 ,average reward of last 25 episode 15.2\n",
      "episode 3100 ,average reward of last 25 episode 17.08\n",
      "episode 3125 ,average reward of last 25 episode 17.16\n",
      "episode 3150 ,average reward of last 25 episode 18.64\n",
      "episode 3175 ,average reward of last 25 episode 19.12\n",
      "episode 3200 ,average reward of last 25 episode 19.56\n",
      "episode 3225 ,average reward of last 25 episode 18.28\n",
      "episode 3250 ,average reward of last 25 episode 19.28\n",
      "episode 3275 ,average reward of last 25 episode 18.16\n",
      "episode 3300 ,average reward of last 25 episode 19.16\n",
      "episode 3325 ,average reward of last 25 episode 19.8\n",
      "episode 3350 ,average reward of last 25 episode 18.76\n",
      "episode 3375 ,average reward of last 25 episode 18.56\n",
      "episode 3400 ,average reward of last 25 episode 18.6\n",
      "episode 3425 ,average reward of last 25 episode 19.8\n",
      "episode 3450 ,average reward of last 25 episode 18.92\n",
      "episode 3475 ,average reward of last 25 episode 20.4\n",
      "episode 3500 ,average reward of last 25 episode 20.76\n",
      "episode 3525 ,average reward of last 25 episode 18.92\n",
      "episode 3550 ,average reward of last 25 episode 18.32\n",
      "episode 3575 ,average reward of last 25 episode 18.64\n",
      "episode 3600 ,average reward of last 25 episode 18.52\n",
      "episode 3625 ,average reward of last 25 episode 19.96\n",
      "episode 3650 ,average reward of last 25 episode 19.76\n",
      "episode 3675 ,average reward of last 25 episode 17.72\n",
      "episode 3700 ,average reward of last 25 episode 18.68\n",
      "episode 3725 ,average reward of last 25 episode 19.56\n",
      "episode 3750 ,average reward of last 25 episode 19.2\n",
      "episode 3775 ,average reward of last 25 episode 21.16\n",
      "episode 3800 ,average reward of last 25 episode 19.96\n",
      "episode 3825 ,average reward of last 25 episode 17.76\n",
      "episode 3850 ,average reward of last 25 episode 18.84\n",
      "episode 3875 ,average reward of last 25 episode 19.36\n",
      "episode 3900 ,average reward of last 25 episode 20.4\n",
      "episode 3925 ,average reward of last 25 episode 19.32\n",
      "episode 3950 ,average reward of last 25 episode 19.36\n",
      "episode 3975 ,average reward of last 25 episode 22.12\n",
      "episode 4000 ,average reward of last 25 episode 19.24\n",
      "Saved Model\n",
      "episode 4025 ,average reward of last 25 episode 20.52\n",
      "episode 4050 ,average reward of last 25 episode 19.4\n",
      "episode 4075 ,average reward of last 25 episode 20.24\n",
      "episode 4100 ,average reward of last 25 episode 19.08\n",
      "episode 4125 ,average reward of last 25 episode 18.92\n",
      "episode 4150 ,average reward of last 25 episode 19.76\n",
      "episode 4175 ,average reward of last 25 episode 19.24\n",
      "episode 4200 ,average reward of last 25 episode 21.08\n",
      "episode 4225 ,average reward of last 25 episode 19.76\n",
      "episode 4250 ,average reward of last 25 episode 18.28\n",
      "episode 4275 ,average reward of last 25 episode 18.36\n",
      "episode 4300 ,average reward of last 25 episode 20.24\n",
      "episode 4325 ,average reward of last 25 episode 19.96\n",
      "episode 4350 ,average reward of last 25 episode 19.0\n",
      "episode 4375 ,average reward of last 25 episode 21.04\n",
      "episode 4400 ,average reward of last 25 episode 19.88\n",
      "episode 4425 ,average reward of last 25 episode 20.44\n",
      "episode 4450 ,average reward of last 25 episode 18.2\n",
      "episode 4475 ,average reward of last 25 episode 19.8\n",
      "episode 4500 ,average reward of last 25 episode 20.12\n",
      "episode 4525 ,average reward of last 25 episode 19.2\n",
      "episode 4550 ,average reward of last 25 episode 19.84\n",
      "episode 4575 ,average reward of last 25 episode 20.44\n",
      "episode 4600 ,average reward of last 25 episode 21.16\n",
      "episode 4625 ,average reward of last 25 episode 20.84\n",
      "episode 4650 ,average reward of last 25 episode 21.52\n",
      "episode 4675 ,average reward of last 25 episode 21.0\n",
      "episode 4700 ,average reward of last 25 episode 21.16\n",
      "episode 4725 ,average reward of last 25 episode 21.2\n",
      "episode 4750 ,average reward of last 25 episode 18.88\n",
      "episode 4775 ,average reward of last 25 episode 20.16\n",
      "episode 4800 ,average reward of last 25 episode 21.64\n",
      "episode 4825 ,average reward of last 25 episode 19.6\n",
      "episode 4850 ,average reward of last 25 episode 19.4\n",
      "episode 4875 ,average reward of last 25 episode 19.48\n",
      "episode 4900 ,average reward of last 25 episode 22.44\n",
      "episode 4925 ,average reward of last 25 episode 21.32\n",
      "episode 4950 ,average reward of last 25 episode 20.44\n",
      "episode 4975 ,average reward of last 25 episode 19.24\n",
      "episode 5000 ,average reward of last 25 episode 20.8\n",
      "Saved Model\n",
      "episode 5025 ,average reward of last 25 episode 22.12\n",
      "episode 5050 ,average reward of last 25 episode 22.24\n",
      "episode 5075 ,average reward of last 25 episode 22.28\n",
      "episode 5100 ,average reward of last 25 episode 18.2\n",
      "episode 5125 ,average reward of last 25 episode 20.64\n",
      "episode 5150 ,average reward of last 25 episode 19.12\n",
      "episode 5175 ,average reward of last 25 episode 19.8\n",
      "episode 5200 ,average reward of last 25 episode 19.44\n",
      "episode 5225 ,average reward of last 25 episode 21.2\n",
      "episode 5250 ,average reward of last 25 episode 20.88\n",
      "episode 5275 ,average reward of last 25 episode 20.48\n",
      "episode 5300 ,average reward of last 25 episode 20.28\n",
      "episode 5325 ,average reward of last 25 episode 20.52\n",
      "episode 5350 ,average reward of last 25 episode 19.28\n",
      "episode 5375 ,average reward of last 25 episode 20.24\n",
      "episode 5400 ,average reward of last 25 episode 21.56\n",
      "episode 5425 ,average reward of last 25 episode 19.48\n",
      "episode 5450 ,average reward of last 25 episode 20.84\n",
      "episode 5475 ,average reward of last 25 episode 21.0\n",
      "episode 5500 ,average reward of last 25 episode 18.4\n",
      "episode 5525 ,average reward of last 25 episode 20.76\n",
      "episode 5550 ,average reward of last 25 episode 19.8\n",
      "episode 5575 ,average reward of last 25 episode 20.32\n",
      "episode 5600 ,average reward of last 25 episode 22.36\n",
      "episode 5625 ,average reward of last 25 episode 20.88\n",
      "episode 5650 ,average reward of last 25 episode 20.16\n",
      "episode 5675 ,average reward of last 25 episode 20.8\n",
      "episode 5700 ,average reward of last 25 episode 19.84\n",
      "episode 5725 ,average reward of last 25 episode 19.76\n",
      "episode 5750 ,average reward of last 25 episode 20.12\n",
      "episode 5775 ,average reward of last 25 episode 20.36\n",
      "episode 5800 ,average reward of last 25 episode 20.72\n",
      "episode 5825 ,average reward of last 25 episode 21.64\n",
      "episode 5850 ,average reward of last 25 episode 19.88\n",
      "episode 5875 ,average reward of last 25 episode 20.8\n",
      "episode 5900 ,average reward of last 25 episode 19.92\n",
      "episode 5925 ,average reward of last 25 episode 20.64\n",
      "episode 5950 ,average reward of last 25 episode 20.08\n",
      "episode 5975 ,average reward of last 25 episode 19.52\n",
      "episode 6000 ,average reward of last 25 episode 20.28\n",
      "Saved Model\n",
      "episode 6025 ,average reward of last 25 episode 21.2\n",
      "episode 6050 ,average reward of last 25 episode 19.96\n",
      "episode 6075 ,average reward of last 25 episode 20.48\n",
      "episode 6100 ,average reward of last 25 episode 19.72\n",
      "episode 6125 ,average reward of last 25 episode 19.48\n",
      "episode 6150 ,average reward of last 25 episode 21.88\n",
      "episode 6175 ,average reward of last 25 episode 20.68\n",
      "episode 6200 ,average reward of last 25 episode 20.48\n",
      "episode 6225 ,average reward of last 25 episode 21.72\n",
      "episode 6250 ,average reward of last 25 episode 22.68\n",
      "episode 6275 ,average reward of last 25 episode 19.32\n",
      "episode 6300 ,average reward of last 25 episode 20.6\n",
      "episode 6325 ,average reward of last 25 episode 20.04\n",
      "episode 6350 ,average reward of last 25 episode 21.68\n",
      "episode 6375 ,average reward of last 25 episode 21.08\n",
      "episode 6400 ,average reward of last 25 episode 21.72\n",
      "episode 6425 ,average reward of last 25 episode 20.64\n",
      "episode 6450 ,average reward of last 25 episode 19.08\n",
      "episode 6475 ,average reward of last 25 episode 20.6\n",
      "episode 6500 ,average reward of last 25 episode 21.88\n",
      "episode 6525 ,average reward of last 25 episode 20.4\n",
      "episode 6550 ,average reward of last 25 episode 21.52\n",
      "episode 6575 ,average reward of last 25 episode 19.0\n",
      "episode 6600 ,average reward of last 25 episode 20.32\n",
      "episode 6625 ,average reward of last 25 episode 19.56\n",
      "episode 6650 ,average reward of last 25 episode 19.04\n",
      "episode 6675 ,average reward of last 25 episode 20.6\n",
      "episode 6700 ,average reward of last 25 episode 23.12\n",
      "episode 6725 ,average reward of last 25 episode 19.88\n",
      "episode 6750 ,average reward of last 25 episode 20.48\n",
      "episode 6775 ,average reward of last 25 episode 19.0\n",
      "episode 6800 ,average reward of last 25 episode 20.08\n",
      "episode 6825 ,average reward of last 25 episode 21.16\n",
      "episode 6850 ,average reward of last 25 episode 22.8\n",
      "episode 6875 ,average reward of last 25 episode 18.24\n",
      "episode 6900 ,average reward of last 25 episode 20.56\n",
      "episode 6925 ,average reward of last 25 episode 20.88\n",
      "episode 6950 ,average reward of last 25 episode 21.08\n",
      "episode 6975 ,average reward of last 25 episode 20.76\n",
      "episode 7000 ,average reward of last 25 episode 20.2\n",
      "Saved Model\n",
      "episode 7025 ,average reward of last 25 episode 21.96\n",
      "episode 7050 ,average reward of last 25 episode 21.2\n",
      "episode 7075 ,average reward of last 25 episode 21.16\n",
      "episode 7100 ,average reward of last 25 episode 22.48\n",
      "episode 7125 ,average reward of last 25 episode 22.32\n",
      "episode 7150 ,average reward of last 25 episode 20.68\n",
      "episode 7175 ,average reward of last 25 episode 20.08\n",
      "episode 7200 ,average reward of last 25 episode 19.64\n",
      "episode 7225 ,average reward of last 25 episode 21.8\n",
      "episode 7250 ,average reward of last 25 episode 21.08\n",
      "episode 7275 ,average reward of last 25 episode 21.4\n",
      "episode 7300 ,average reward of last 25 episode 21.28\n",
      "episode 7325 ,average reward of last 25 episode 20.32\n",
      "episode 7350 ,average reward of last 25 episode 20.64\n",
      "episode 7375 ,average reward of last 25 episode 20.4\n",
      "episode 7400 ,average reward of last 25 episode 19.4\n",
      "episode 7425 ,average reward of last 25 episode 20.32\n",
      "episode 7450 ,average reward of last 25 episode 21.8\n",
      "episode 7475 ,average reward of last 25 episode 20.64\n",
      "episode 7500 ,average reward of last 25 episode 17.8\n",
      "episode 7525 ,average reward of last 25 episode 20.48\n",
      "episode 7550 ,average reward of last 25 episode 20.52\n",
      "episode 7575 ,average reward of last 25 episode 21.44\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    if load_model == True:\n",
    "        print(\"Loading Model...\")\n",
    "        ckpt=tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    sess.run(init)\n",
    "    updateTarget(targetOps,sess)\n",
    "    \n",
    "    for i in range(num_episodes+1):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        s = env.reset()\n",
    "        s = flattenState(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        while j < max_epLength:\n",
    "            j+=1\n",
    "            if np.random.rand(1) < e or total_steps <pre_train_steps:\n",
    "                a =np.random.randint(0,4)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict,\n",
    "                            feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "            s1,r,d=env.step(a)\n",
    "            s1=flattenState(s1)\n",
    "            total_steps +=1\n",
    "            episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5]))\n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e-=stepDrop\n",
    "                if total_steps % update_freq == 0:\n",
    "                    trainBatch =buff.sample(batch_size)\n",
    "                    A = sess.run(mainQN.predict,feed_dict={\n",
    "                            mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    Q = sess.run(targetQN.Qout,feed_dict={\n",
    "                            targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    doubleQ = Q[range(batch_size),A]\n",
    "                    targetQ = trainBatch[:,2] + y*doubleQ\n",
    "                    _ = sess.run(mainQN.updateModel,feed_dict={\n",
    "                            mainQN.scalarInput:np.vstack(trainBatch[:,0]),\n",
    "                            mainQN.targetQ:targetQ,\n",
    "                            mainQN.actions:trainBatch[:,1]\n",
    "                        })\n",
    "                    updateTarget(targetOps,sess) \n",
    "            rAll += r\n",
    "            s = s1\n",
    "            \n",
    "            if d:\n",
    "                break\n",
    "        buff.add(episodeBuffer.buffer)\n",
    "        rList.append(rAll)\n",
    "        if i>0 and i%25 == 0:\n",
    "            print('episode',i,',average reward of last 25 episode',\n",
    "                 np.mean(rList[-25:]))\n",
    "        if i>0 and i % 1000==0:\n",
    "            saver.save (sess,path+'/model-'+str(i)+'.cptk')\n",
    "            print(\"Saved Model\")\n",
    "    saver.save (sess,path+'/model-'+str(i)+'.cptk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "iterables = [range(4),range(4)]\n",
    "points = []\n",
    "for t in itertools.product(range(4),range(4)):\n",
    "    points.append(t)\n",
    "points.remove((1,1))    \n",
    "# print(\"points:\",points)\n",
    "\n",
    "a = np.ones([4,4,3])\n",
    "a[1:-1,1:-1,:] = 0\n",
    "# print(\"a:\",a)\n",
    "\n",
    "import tensorflow as tf\n",
    "a = tf.ones([4,2,2,3])\n",
    "streamA=tf.contrib.layers.flatten(a)\n",
    "# sess=tf.InteractiveSession()\n",
    "print(\"a:\",streamA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
